{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HPC-Inference","text":"HPC-Inference <p>Batch inference solution for large-scale image datasets on HPC</p>"},{"location":"#about","title":"About","text":"<p>Problem: Many batch inference workflows waste GPU resources due to I/O bottlenecks and sequential processing, leading to poor GPU utilization and longer processing times.</p>"},{"location":"#key-bottlenecks","title":"Key Bottlenecks","text":"<ul> <li>Slow sequential large file loading (Disk \u2192 RAM)</li> <li>Single-threaded image preprocessing</li> <li>Data transfer delays (CPU \u2194 GPU)</li> <li>GPU idle time waiting for data</li> <li>Sequential output writing</li> </ul>"},{"location":"#hpc-inference-solutions","title":"HPC-Inference Solutions","text":"<ul> <li>Parallel data loading: Eliminates disk I/O bottlenecks with optimized dataset loaders</li> <li>Asynchronous preprocessing: Keeps GPUs fed with continuous data queues  </li> <li>SLURM integration: Deploy seamlessly on HPC clusters</li> <li>Multi-GPU distribution: Scales across HPC nodes for maximum throughput</li> <li>Resource profiling: Logs timing metrics and CPU/GPU usage rates to help optimize your configuration</li> </ul>"},{"location":"#core-features","title":"Core Features","text":"<p>The <code>hpc_inference</code> package's core functionality includes customized PyTorch datasets:</p> <ul> <li><code>ParquetImageDataset</code> for image data stored as compressed binary columns across multiple large Parquet files</li> <li><code>ImageFolderDataset</code> for image data stored in folders using open file formats such as <code>JPG</code>, <code>PNG</code>, <code>TIFF</code>, etc.</li> </ul> <p>The package also comes with a suite of ready-to-use job scripts to perform efficient batch inference using pretrained models on HPCs.</p>"},{"location":"#use-cases","title":"Use Cases","text":"<ol> <li>Image Folder Dataset - Process images from directory structures</li> <li>Parquet Dataset - Handle compressed image data in Parquet format</li> <li>Large scale CLIP embedding - Generate embeddings for massive datasets</li> <li>Large scale face detection - Detect faces across large image collections</li> <li>Large scale animal detection - Use MegaDetector for wildlife analysis</li> <li>Grid search profiling - Optimize processing parameters</li> </ol>"},{"location":"#quick-links","title":"Quick Links","text":""},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>This project is a joint effort between the Imageomics Institute and the ABC Global Center.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>This page provides comprehensive API documentation for the HPC-Inference package.</p>"},{"location":"api-reference/#hpc_inference.datasets","title":"<code>hpc_inference.datasets</code>","text":"<p>High-performance dataset loaders for image data.</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset","title":"<code>ImageFolderDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Loads images from a folder in a streaming fashion, with support for distributed processing. - Handles common image extensions - Loads images as RGB by default (can be changed via color_mode) - Validates images using PIL if validate=True - Supports distributed processing with rank-based file partitioning - Supports multi-worker data loading within each rank</p> <p>Returns (uuid, processed_data) tuples where: - uuid: filename (without path) as identifier - processed_data: tensor (single model) or dict of tensors (multi-model)</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.__init__","title":"<code>__init__(image_dir, preprocess=None, color_mode='RGB', validate=False, rank=None, world_size=None, evenly_distribute=True, stagger=False, uuid_mode='filename')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>image_dir</code> <code>Union[str, Path]</code> <p>Path to image folder.</p> required <code>preprocess</code> <code>Optional[Union[Callable, Dict[str, Callable]]]</code> <p>Transform(s) to apply to images. - If callable: single model preprocessing - If dict: {model_name: preprocess_fn} for multi-model - If None: return PIL image as-is</p> <code>None</code> <code>color_mode</code> <code>str</code> <p>Color mode for PIL.Image.convert (default: \"RGB\").</p> <code>'RGB'</code> <code>validate</code> <code>bool</code> <p>If True, validates images in the directory using PIL.</p> <code>False</code> <code>rank</code> <code>Optional[int]</code> <p>Current process rank for distributed processing.</p> <code>None</code> <code>world_size</code> <code>Optional[int]</code> <p>Total number of processes for distributed processing.</p> <code>None</code> <code>evenly_distribute</code> <code>bool</code> <p>Whether to distribute files evenly based on size. Defaults to True. If False, files are distributed in a round-robin manner.</p> <code>True</code> <code>stagger</code> <code>bool</code> <p>Whether to stagger the start of each worker. Defaults to False.</p> <code>False</code> <code>uuid_mode</code> <code>Literal['filename', 'relative', 'fullpath', 'hash']</code> <p>How to generate UUIDs from image paths. - \"filename\": Use just the filename (image001.jpg) - \"relative\": Use relative path from image_dir (subfolder/image001.jpg) - \"fullpath\": Use full absolute path - \"hash\": Use hash of the full path</p> <code>'filename'</code>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over images, yielding (uuid, processed_image) tuples. Supports multi-worker data loading within each rank.</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return approximate length (actual length depends on worker assignment).</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.parse_image","title":"<code>parse_image(img_path)</code>","text":"<p>Load and process a single image.</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.validate_PIL","title":"<code>validate_PIL(file_path)</code>  <code>staticmethod</code>","text":"<p>Validates if the file can be opened by PIL. Returns True if valid, False otherwise.</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.validate_image_files","title":"<code>validate_image_files(image_files, max_workers=16)</code>  <code>classmethod</code>","text":"<p>Validates a list of image files using PIL.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetEmbeddingDataset","title":"<code>ParquetEmbeddingDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Loads pre-computed embeddings from Parquet files in a streaming fashion. - Reads embedding vectors from Parquet files - Supports distributed processing with rank-based file partitioning - Supports multi-worker data loading within each rank - Optimized for loading numerical data (embeddings) rather than images</p> <p>Returns (uuid, embedding) tuples where: - uuid: Unique identifier from the UUID column in Parquet - embedding: Numpy array or tensor containing the embedding vector</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetEmbeddingDataset.__init__","title":"<code>__init__(parquet_files, col_uuid='uuid', col_embedding='embedding', rank=None, world_size=None, evenly_distribute=True, read_batch_size=1000, read_columns=None, stagger=False, return_tensor=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>parquet_files</code> <code>List[Union[str, Path]]</code> <p>List of paths to Parquet files containing embedding data.</p> required <code>col_uuid</code> <code>str</code> <p>Name of the UUID column in Parquet files. Defaults to \"uuid\".</p> <code>'uuid'</code> <code>col_embedding</code> <code>str</code> <p>Name of the embedding column in Parquet files. Defaults to \"embedding\".</p> <code>'embedding'</code> <code>rank</code> <code>Optional[int]</code> <p>Current process rank for distributed processing.</p> <code>None</code> <code>world_size</code> <code>Optional[int]</code> <p>Total number of processes for distributed processing.</p> <code>None</code> <code>evenly_distribute</code> <code>bool</code> <p>Whether to distribute files evenly based on size. Defaults to True. If False, files are distributed in a round-robin manner.</p> <code>True</code> <code>read_batch_size</code> <code>int</code> <p>Number of rows to read from Parquet at a time. Defaults to 1000.</p> <code>1000</code> <code>read_columns</code> <code>Optional[List[str]]</code> <p>List of column names to read from Parquet. If None, reads all columns. Typically includes [\"uuid\", \"embedding\"].</p> <code>None</code> <code>stagger</code> <code>bool</code> <p>Whether to stagger the start of each worker. Defaults to False.</p> <code>False</code> <code>return_tensor</code> <code>bool</code> <p>If True, convert embeddings to PyTorch tensors. If False, keep as numpy arrays.</p> <code>True</code>"},{"location":"api-reference/#hpc_inference.datasets.ParquetEmbeddingDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over Parquet files and yield embedding data. Supports multi-worker data loading within each rank.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetEmbeddingDataset.process_parquet_file","title":"<code>process_parquet_file(file_path)</code>","text":"<p>Process a single Parquet file and yield embedding data.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the Parquet file to process.</p> required <p>Yields:</p> Type Description <code>Tuple[str, Any]</code> <p>Tuples of (uuid, embedding) for each row in the file.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset","title":"<code>ParquetImageDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Loads images from Parquet files in a streaming fashion, with support for distributed processing. - Reads image data from Parquet files containing encoded image bytes - Supports distributed processing with rank-based file partitioning - Supports multi-worker data loading within each rank - Handles both single-model and multi-model preprocessing - Provides staggered worker starts and load balancing</p> <p>Returns (uuid, processed_data) tuples where: - uuid: Unique identifier from the UUID column in Parquet - processed_data: Tensor (single model) or dict of tensors (multi-model)</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset.__init__","title":"<code>__init__(parquet_files, col_uuid='uuid', rank=None, world_size=None, evenly_distribute=True, decode_fn=None, preprocess=None, read_batch_size=128, read_columns=None, stagger=False, processed_files_log=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>parquet_files</code> <code>List[Union[str, Path]]</code> <p>List of paths to Parquet files containing image data.</p> required <code>col_uuid</code> <code>str</code> <p>Name of the UUID column in Parquet files. Defaults to \"uuid\".</p> <code>'uuid'</code> <code>rank</code> <code>Optional[int]</code> <p>Current process rank for distributed processing.</p> <code>None</code> <code>world_size</code> <code>Optional[int]</code> <p>Total number of processes for distributed processing.</p> <code>None</code> <code>evenly_distribute</code> <code>bool</code> <p>Whether to distribute files evenly based on size. Defaults to True. If False, files are distributed in a round-robin manner.</p> <code>True</code> <code>decode_fn</code> <code>Optional[Callable]</code> <p>Function to decode image bytes to PIL Image. Required for image processing.</p> <code>None</code> <code>preprocess</code> <code>Optional[Union[Callable, Dict[str, Callable]]]</code> <p>Transform(s) to apply to images. - If callable: single model preprocessing - If dict: {model_name: preprocess_fn} for multi-model - If None: return decoded image as-is</p> <code>None</code> <code>read_batch_size</code> <code>int</code> <p>Number of rows to read from Parquet at a time. Defaults to 128.</p> <code>128</code> <code>read_columns</code> <code>Optional[List[str]]</code> <p>List of column names to read from Parquet. If None, reads all columns. Typically includes [\"uuid\", \"image\", \"original_size\", \"resized_size\"].</p> <code>None</code> <code>stagger</code> <code>bool</code> <p>Whether to stagger the start of each worker. Defaults to False.</p> <code>False</code> <code>processed_files_log</code> <code>Optional[Union[str, Path]]</code> <p>Path to log file for tracking processed files. Optional.</p> <code>None</code>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over Parquet files and yield processed data. Supports multi-worker data loading within each rank.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset.log_processed_file","title":"<code>log_processed_file(file_path)</code>","text":"<p>Log a processed file to the log file if configured.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset.parse_batch_data","title":"<code>parse_batch_data(batch_df)</code>","text":"<p>Parse a batch of data from Parquet DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>batch_df</code> <code>DataFrame</code> <p>DataFrame containing batch data from Parquet file.</p> required <p>Yields:</p> Type Description <code>Tuple[str, Any]</code> <p>Tuples of (uuid, processed_data) for each row in the batch.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset.process_parquet_file","title":"<code>process_parquet_file(file_path)</code>","text":"<p>Process a single Parquet file and yield processed data.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the Parquet file to process.</p> required <p>Yields:</p> Type Description <code>Tuple[str, Any]</code> <p>Tuples of (uuid, processed_data) for each valid row in the file.</p>"},{"location":"api-reference/#hpc_inference.datasets.multi_model_collate","title":"<code>multi_model_collate(batch)</code>","text":"<p>Collate function for batches where each sample is (uuid, {model_name: tensor, ...}).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tuple[str, Dict[str, Any]]]</code> <p>List of (uuid, processed_data_dict) tuples from dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], Dict[str, Tensor]]</code> <p>Tuple containing: - uuids: List of UUID strings - batch_dict: Dict mapping model names to batched tensors</p>"},{"location":"api-reference/#hpc_inference.inference","title":"<code>hpc_inference.inference</code>","text":"<p>Inference modules for various model types.</p>"},{"location":"api-reference/#hpc_inference.utils","title":"<code>hpc_inference.utils</code>","text":"<p>Utility functions for HPC inference.</p>"},{"location":"api-reference/#hpc_inference.utils.assign_files_to_rank","title":"<code>assign_files_to_rank(rank, world_size, files, evenly_distribute=True)</code>","text":"<p>Assign files to the current rank based on the world size.</p> <p>This method ensures that each rank gets a unique set of files to process. The files can be distributed evenly based on their size (LPT algorithm) or simply by their order. This is useful for large datasets where some files may be significantly larger than others.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Current process rank (0-indexed).</p> required <code>world_size</code> <code>int</code> <p>Total number of processes across all ranks.</p> required <code>files</code> <code>List[Union[str, Path]]</code> <p>List of file paths to distribute across ranks.</p> required <code>evenly_distribute</code> <code>bool</code> <p>Whether to distribute files evenly based on file size. Defaults to True. - If True: Uses Longest Processing Time (LPT) algorithm for load balancing - If False: Uses simple round-robin distribution</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths assigned to the given rank.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If rank is negative or &gt;= world_size.</p> <code>FileNotFoundError</code> <p>If any file in the list doesn't exist when evenly_distribute=True.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; files = [\"/path/file1.parquet\", \"/path/file2.parquet\", \"/path/file3.parquet\"]\n&gt;&gt;&gt; assign_files_to_rank(0, 2, files, evenly_distribute=False)\n[\"/path/file1.parquet\", \"/path/file3.parquet\"]\n</code></pre> <pre><code>&gt;&gt;&gt; assign_files_to_rank(1, 2, files, evenly_distribute=False) \n[\"/path/file2.parquet\"]\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.assign_indices_to_rank","title":"<code>assign_indices_to_rank(rank, world_size, total_items, evenly_distribute=True)</code>","text":"<p>Assign item indices to the current rank based on the world size.</p> <p>This function calculates which range of indices each rank should process when working with datasets that can be split by index ranges.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Current process rank (0-indexed).</p> required <code>world_size</code> <code>int</code> <p>Total number of processes across all ranks.</p> required <code>total_items</code> <code>int</code> <p>Total number of items to distribute across ranks.</p> required <code>evenly_distribute</code> <code>bool</code> <p>Whether to distribute items evenly. Defaults to True. - If True: Each rank gets approximately equal number of items - If False: Simple round-robin style distribution</p> <code>True</code> <p>Returns:</p> Type Description <code>int</code> <p>Tuple of (start_idx, end_idx) representing the range of indices for this rank.</p> <code>int</code> <p>The range is [start_idx, end_idx) (end_idx is exclusive).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If rank is negative or &gt;= world_size, or if total_items is negative.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; assign_indices_to_rank(0, 3, 10, evenly_distribute=True)\n(0, 4)  # Rank 0 gets indices 0,1,2,3\n</code></pre> <pre><code>&gt;&gt;&gt; assign_indices_to_rank(1, 3, 10, evenly_distribute=True) \n(4, 7)  # Rank 1 gets indices 4,5,6\n</code></pre> <pre><code>&gt;&gt;&gt; assign_indices_to_rank(2, 3, 10, evenly_distribute=True)\n(7, 10) # Rank 2 gets indices 7,8,9\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.decode_image","title":"<code>decode_image(row_data, return_type='pil')</code>","text":"<p>Decode an image stored as raw bytes in a row dictionary into a NumPy array or PIL Image.</p> <p>The function expects the row dictionary to contain an \"image\" key with raw image bytes, and either an \"original_size\" or \"resized_size\" key specifying the (height, width) of the image.</p> <p>It assumes the image has RGB channels, and reshapes the byte buffer accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>row_data</code> <code>Union[Dict[str, Any], Series]</code> <p>Dictionary containing image data and metadata. - \"image\": Raw image bytes (bytes or bytearray). - \"original_size\" or \"resized_size\": Tuple (height, width) of the image.</p> required <code>return_type</code> <code>Literal['numpy', 'pil']</code> <p>Type of the returned image. Can be \"numpy\" for NumPy array or \"pil\" for PIL Image.</p> <code>'pil'</code> <p>Returns:</p> Type Description <code>Optional[Union[ndarray, Image]]</code> <p>Decoded image as a NumPy array (RGB format) or PIL Image, or None if decoding fails.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns None on any decoding error, with warning logged.</p> Notes <ul> <li>Returns None if the image size does not match the expected dimensions.</li> <li>Logs a warning if decoding fails.</li> <li>Converts BGR to RGB channel order automatically.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; row = {\"image\": image_bytes, \"original_size\": (224, 224)}\n&gt;&gt;&gt; img = decode_image(row, return_type=\"pil\")\n&gt;&gt;&gt; isinstance(img, Image.Image)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; img_array = decode_image(row, return_type=\"numpy\")\n&gt;&gt;&gt; img_array.shape\n(224, 224, 3)\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.format_time","title":"<code>format_time(seconds)</code>","text":"<p>Format seconds into a human-readable time string.</p> <p>Parameters:</p> Name Type Description Default <code>seconds</code> <code>float</code> <p>Time duration in seconds.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted time string (e.g., \"1h 30m 45.67s\", \"2m 15.34s\", \"12.45s\").</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; format_time(3661.5)\n'1h 1m 1.50s'\n&gt;&gt;&gt; format_time(125.67)\n'2m 5.67s'\n&gt;&gt;&gt; format_time(45.23)\n'45.23s'\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.get_distributed_info","title":"<code>get_distributed_info()</code>","text":"<p>Get distributed training information from environment variables.</p> <p>This function extracts rank and world size from common distributed training environment variables (SLURM, torchrun, etc.).</p> <p>Returns:</p> Type Description <code>int</code> <p>Tuple of (rank, world_size) extracted from environment variables.</p> <code>int</code> <p>If no distributed environment is detected, returns (0, 1).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # In SLURM environment\n&gt;&gt;&gt; get_distributed_info()\n(2, 8)  # rank=2, world_size=8\n</code></pre> <pre><code>&gt;&gt;&gt; # In single process environment  \n&gt;&gt;&gt; get_distributed_info()\n(0, 1)  # rank=0, world_size=1\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.load_config","title":"<code>load_config(config_path)</code>","text":"<p>Load YAML configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Union[str, Path]</code> <p>Path to YAML configuration file.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Parsed configuration as a dictionary.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the configuration file doesn't exist.</p> <code>YAMLError</code> <p>If the YAML file is malformed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = load_config(\"config.yaml\")\n&gt;&gt;&gt; print(config[\"batch_size\"])\n32\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.multi_model_collate","title":"<code>multi_model_collate(batch)</code>","text":"<p>Collate function for batches where each sample is (uuid, {model_name: tensor, ...}).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tuple[str, Dict[str, Any]]]</code> <p>List of (uuid, processed_data_dict) tuples from dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], Dict[str, Tensor]]</code> <p>Tuple containing: - uuids: List of UUID strings - batch_dict: Dict mapping model names to batched tensors</p>"},{"location":"api-reference/#hpc_inference.utils.pil_image_collate","title":"<code>pil_image_collate(batch)</code>","text":"<p>Custom collate function for batches containing PIL Images.</p> <p>This function is required when working with datasets that return PIL Images because PyTorch's default collate function only handles tensors, numpy arrays, numbers, dicts, and lists - not PIL Image objects.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tuple[str, Any]]</code> <p>List of (uuid, image) tuples where image is a PIL Image.    Each tuple contains a UUID string and a PIL Image object.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[Any]]</code> <p>Tuple containing: - uuids: List of UUID strings from the batch - images: List of PIL Image objects from the batch</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; batch = [(\"img1.jpg\", Image.new(\"RGB\", (100, 100))), \n...           (\"img2.jpg\", Image.new(\"RGB\", (200, 200)))]\n&gt;&gt;&gt; uuids, images = pil_image_collate(batch)\n&gt;&gt;&gt; print(uuids)\n['img1.jpg', 'img2.jpg']\n&gt;&gt;&gt; print([img.size for img in images])\n[(100, 100), (200, 200)]\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.save_emb_to_parquet","title":"<code>save_emb_to_parquet(uuids, embeddings, path, compression='zstd')</code>","text":"<p>Save embeddings (as a dict of column names to tensors/arrays) and uuids to a Parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>uuids</code> <code>List[str]</code> <p>List of unique identifiers corresponding to each embedding.</p> required <code>embeddings</code> <code>Dict[str, Union[Tensor, ndarray]]</code> <p>Dictionary mapping column names to torch.Tensor or np.ndarray. Each tensor/array should have the same first dimension as len(uuids).</p> required <code>path</code> <code>Union[str, Path]</code> <p>Output Parquet file path.</p> required <code>compression</code> <code>str</code> <p>Compression type for Parquet file. Defaults to \"zstd\". Other options: \"snappy\", \"gzip\", \"brotli\", \"lz4\", \"uncompressed\".</p> <code>'zstd'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If embeddings have mismatched dimensions or empty inputs.</p> <code>IOError</code> <p>If file cannot be written.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; uuids = [\"img_001\", \"img_002\", \"img_003\"]\n&gt;&gt;&gt; embeddings = {\n...     \"clip_emb\": torch.randn(3, 512),\n...     \"resnet_emb\": torch.randn(3, 2048)\n... }\n&gt;&gt;&gt; save_emb_to_parquet(uuids, embeddings, \"embeddings.parquet\")\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.validate_distributed_setup","title":"<code>validate_distributed_setup(rank, world_size)</code>","text":"<p>Validate distributed training setup parameters.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Process rank to validate.</p> required <code>world_size</code> <code>int</code> <p>World size to validate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if setup is valid, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid.</p>"},{"location":"api-reference/#quick-reference","title":"Quick Reference","text":""},{"location":"api-reference/#core-classes","title":"Core Classes","text":"<ul> <li><code>ImageFolderDataset</code> - Process images from directory structures</li> <li><code>ParquetImageDataset</code> - Handle compressed image data in Parquet format</li> </ul>"},{"location":"api-reference/#utility-functions","title":"Utility Functions","text":"<ul> <li>GPU utilization monitoring functions</li> <li>Performance profiling utilities</li> <li>SLURM integration helpers</li> </ul>"},{"location":"api-reference/#configuration","title":"Configuration","text":"<ul> <li>Model configuration templates</li> <li>Preprocessing pipelines</li> <li>Batch processing optimization</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#setup-with-uv","title":"Setup with uv","text":"<pre><code># Clone Repository\ngit clone https://github.com/Imageomics/hpc-inference.git\ncd hpc-inference\n\n# Install uv (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment and install package\nuv venv hpc-inference-env\nsource hpc-inference-env/bin/activate\n\n# Install base package\nuv pip install -e .\n</code></pre>"},{"location":"getting-started/#verify-installation","title":"Verify Installation","text":"<pre><code># Test the installation\npython -c \"import hpc_inference; print('\u2713 HPC-Inference installed successfully')\"\n</code></pre>"},{"location":"getting-started/#optional-dependencies","title":"Optional Dependencies","text":"<p>The package comes with ready-to-use job scripts for various use cases. Install additional dependencies based on your needs:</p> <pre><code># Check installation status and available features\npython -c \"from hpc_inference import print_installation_guide; print_installation_guide()\"\n\n# Install specific feature sets\nuv pip install -e \".[openclip]\"     # For CLIP embedding\nuv pip install -e \".[detection]\"    # For face/animal detection  \nuv pip install -e \".[all]\"          # Install dependency for all use cases\n</code></pre>"},{"location":"getting-started/#quick-start-next-steps","title":"Quick Start &amp; Next Steps","text":"<ul> <li>Check out the ImageFolderDataset Guide for hands-on examples with scientific image datasets</li> <li>Explore the API Reference for detailed documentation</li> </ul>"},{"location":"imagefolder-guide/","title":"ImageFolderDataset Guide: Working with NEON Beetle Images","text":"<p>This guide demonstrates how to use the <code>ImageFolderDataset</code> class from the HPC Inference package with the 2018 NEON Beetles dataset.</p>"},{"location":"imagefolder-guide/#overview","title":"Overview","text":"<p>The <code>ImageFolderDataset</code> is designed for efficient streaming of large image collections, with support for:</p> <ul> <li>Distributed processing across multiple workers/ranks</li> <li>Multiple preprocessing pipelines for different models</li> <li>Flexible UUID generation from file paths</li> <li>Image validation using PIL</li> <li>Memory-efficient streaming for large datasets</li> </ul>"},{"location":"imagefolder-guide/#dataset-overview","title":"Dataset Overview","text":"<p>The 2018 NEON Beetles dataset contains: - 577 high-resolution group images of ethanol-preserved beetles - Beetles collected from NEON ecological sites in 2018 - Multiple beetles per image, organized by species - Rich metadata including species labels, site information, and measurements</p> <p>We'll work with the <code>group_images</code> folder which contains the original full-size images.</p>"},{"location":"imagefolder-guide/#setup-and-installation","title":"Setup and Installation","text":"<p>First, install the required packages:</p> <pre><code># Install required packages (if not already installed)\n# Follow instruction to install hpc_inference\n# pip install datasets huggingface_hub torch torchvision open_clip_torch pillow matplotlib\n\n# Import required libraries\nimport os\nimport torch\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport pandas as pd\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nimport numpy as np\n</code></pre>"},{"location":"imagefolder-guide/#download-the-dataset","title":"Download the Dataset","text":"<p>Download the NEON Beetles dataset using the Hugging Face datasets library:</p> <p><pre><code>from datasets import load_dataset\nfrom huggingface_hub import snapshot_download\n\n# Download the full dataset (this might take a while - ~5GB)\n# We'll focus on the group_images folder\ndata_dir = \"./neon_beetles_data\"\n\n# Download the dataset\nsnapshot_download(\n    repo_id=\"imageomics/2018-NEON-beetles\",\n    repo_type=\"dataset\",\n    local_dir=data_dir,\n    allow_patterns=[\"group_images/*\"]  # Only download group images\n)\n\n# Set up paths\nimage_dir = Path(data_dir) / \"group_images\"\nprint(f\"Dataset downloaded to: {image_dir}\")\nprint(f\"Number of images: {len(list(image_dir.glob('*.jpg')))}\")\n</code></pre> <pre><code>Fetching 578 files: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 578/578 [00:00&lt;00:00, 635.80it/s]\nDataset downloaded to: neon_beetles_data/group_images\nNumber of images: 577\n</code></pre></p>"},{"location":"imagefolder-guide/#import-imagefolderdataset","title":"Import ImageFolderDataset","text":"<pre><code># Import the ImageFolderDataset from the installed package\nfrom hpc_inference.datasets.image_folder_dataset import ImageFolderDataset\n\n# Import the collate functions from distributed utils\nfrom hpc_inference.utils.distributed import pil_image_collate\n</code></pre>"},{"location":"imagefolder-guide/#basic-usage","title":"Basic Usage","text":"<p>Let's start with the simplest usage - loading images without any preprocessing.</p> <p>Important Note: When using <code>preprocess=None</code> (returning PIL Images), we need a custom collate function because PyTorch's default collate function only knows how to handle tensors, numpy arrays, numbers, dicts, and lists - not PIL Image objects.</p> <p><pre><code># Create a basic dataset (no preprocessing)\nbasic_dataset = ImageFolderDataset(\n    image_dir=image_dir,\n    preprocess=None,  # No preprocessing - returns PIL Images\n    uuid_mode=\"filename\"  # Use filename as UUID\n)\n\nprint(f\"Dataset contains {len(basic_dataset)} images\")\n\n# Create a DataLoader with custom collate function for PIL Images\n# Note: We need a custom collate function because PyTorch's default collate \n# function doesn't know how to handle PIL Image objects\nbasic_loader = DataLoader(\n    basic_dataset, \n    batch_size=2, \n    num_workers=1,\n    collate_fn=pil_image_collate  # Use our custom collate function\n)\n\n# Get first batch\nfor uuids, images in basic_loader:\n    print(f\"Batch UUIDs: {uuids}\")\n    print(f\"Image types: {[type(img) for img in images]}\")\n    print(f\"Image sizes: {[img.size for img in images]}\")\n    break\n</code></pre> <pre><code>2025-07-11 15:22:24,047 - INFO - Rank 0 assigned 577 out of 577 images\nDataset contains 577 images\n2025-07-11 15:22:24,072 - INFO - [Rank 0/Worker 0] Processing 577 images\nBatch UUIDs: ['A00000001831.jpg', 'A00000003356.jpg']\nImage types: [&lt;class 'PIL.Image.Image'&gt;, &lt;class 'PIL.Image.Image'&gt;]\nImage sizes: [(5568, 3712), (5568, 3712)]\n</code></pre> The output above shows that rank 0 (single/current process) was assigned all 577 images from the NEON beetle dataset. A single worker processes these images, returning PIL Image objects with original high-resolution dimensions (5568\u00d73712). The batch contains 2 images with filenames as UUIDs, demonstrating successful operation with the custom <code>pil_image_collate</code> function required for PIL images.</p>"},{"location":"imagefolder-guide/#image-validation","title":"Image Validation","text":"<p>When working with large datasets from unknown sources, you might want to validate that all images can be properly loaded:</p> <p><pre><code># Create dataset with validation enabled\n# Note: This will be slower as it validates each image\nvalidated_dataset = ImageFolderDataset(\n    image_dir=image_dir,\n    preprocess=None,\n    validate=True,  # Enable validation\n    uuid_mode=\"filename\"\n)\n\nprint(f\"Validated dataset contains {len(validated_dataset)} valid images\")\n\n# Compare with non-validated count\ntotal_jpg_files = len(list(image_dir.glob('*.jpg')))\nprint(f\"Total .jpg files in directory: {total_jpg_files}\")\nprint(f\"Valid images after validation: {len(validated_dataset)}\")\n\nif len(validated_dataset) &lt; total_jpg_files:\n    print(f\"{total_jpg_files - len(validated_dataset)} images failed validation\")\nelse:\n    print(\"All images passed validation\")\n</code></pre> <pre><code>2025-07-11 15:33:15,081 - INFO - Rank 0 assigned 577 out of 577 images\nValidated dataset contains 577 valid images\nTotal .jpg files in directory: 577\nValid images after validation: 577\nAll images passed validation\n</code></pre> All of our image downloads are validated! Awesome!</p> <p>Performance Note</p> <p>Image validation will slow down the dataset initialization process. For GPU-intensive workflows, it's recommended to validate your dataset in a separate preprocessing step before submitting jobs that require GPU resources, as GPU idle time during validation can be costly and wasteful.</p>"},{"location":"imagefolder-guide/#single-model-preprocessing","title":"Single Model Preprocessing","text":"<p>Now let's add preprocessing for a single computer vision model. We'll use a simple ResNet preprocessing pipeline:</p> <p><pre><code># Define preprocessing for a single model (e.g., ResNet)\nresnet_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\n# Create dataset with single model preprocessing\nsingle_model_dataset = ImageFolderDataset(\n    image_dir=image_dir,\n    preprocess=resnet_transform,\n    uuid_mode=\"filename\",\n    validate=True  # Set to True for safer operation with unknown image quality\n)\n\n# Create DataLoader\nsingle_loader = DataLoader(single_model_dataset, batch_size=4, num_workers=1)\n\n# Get a batch and examine the output\nfor uuids, tensor_batch in single_loader:\n    print(f\"Batch UUIDs: {uuids}\")\n    print(f\"Tensor batch shape: {tensor_batch.shape}\")\n    print(f\"Tensor dtype: {tensor_batch.dtype}\")\n    print(f\"Tensor range: [{tensor_batch.min():.3f}, {tensor_batch.max():.3f}]\")\n    break\n</code></pre> <pre><code>2025-07-11 15:22:50,871 - INFO - Rank 0 assigned 577 out of 577 images\n2025-07-11 15:22:50,891 - INFO - [Rank 0/Worker 0] Processing 577 images\nBatch UUIDs: ('A00000001831.jpg', 'A00000003356.jpg', 'A00000008914.jpg', 'A00000008915.jpg')\nTensor batch shape: torch.Size([4, 3, 224, 224])\nTensor dtype: torch.float32\nTensor range: [-1.998, 2.588]\n</code></pre> With preprocessing enabled, the dataset now applies the ResNet transformation pipeline to each image. The output shows that our high-resolution NEON beetle images (originally 5568\u00d73712) have been resized to 224\u00d7224 pixels, converted to normalized tensors with 3 color channels. </p> <p>Tensor structure:</p> <ul> <li>Shape <code>[4, 3, 224, 224]</code>: This follows the standard PyTorch convention (batch_size, channels, height, width)</li> <li><code>4</code>: Batch size (4 images in this batch)</li> <li><code>3</code>: Color channels (RGB)</li> <li><code>224, 224</code>: Image dimensions after preprocessing (height, width)</li> <li>Data type <code>torch.float32</code>: Standard floating-point format for neural network input</li> <li>Value range <code>[-1.998, 2.588]</code>: Result normalization with mean=[0.485, 0.456, 0.406] and std=[0.229, 0.224, 0.225]</li> </ul> <p>Notice that we no longer need a custom collate function since PyTorch's default collate function handles tensors natively, making the batch processing more straightforward and efficient than with raw PIL images.</p>"},{"location":"imagefolder-guide/#multi-model-preprocessing","title":"Multi-Model Preprocessing","text":"<p>The <code>ImageFolderDataset</code> supports multiple model preprocessing pipelines simultaneously:</p> <p><pre><code># Define preprocessing for multiple models\nimport open_clip\n\n# Get CLIP preprocessing functions\n_, _, clip_preprocess_vit_b = open_clip.create_model_and_transforms('ViT-B-32', pretrained='openai')\n_, _, clip_preprocess_vit_l = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n\n# Create multi-model preprocessing dictionary\nmulti_model_preprocess = {\n    \"clip_vit_b32\": clip_preprocess_vit_b,\n    \"clip_vit_l14\": clip_preprocess_vit_l,\n    \"resnet\": resnet_transform\n}\n\n# Create dataset with multi-model preprocessing\nmulti_model_dataset = ImageFolderDataset(\n    image_dir=image_dir,\n    preprocess=multi_model_preprocess,\n    uuid_mode=\"relative\",  # Use relative path as UUID\n    validate=False\n)\n\nprint(f\"Multi-model dataset created with {len(multi_model_preprocess)} preprocessing pipelines\")\n</code></pre> <pre><code>2025-07-11 15:23:28,363 - INFO - Loaded ViT-B-32 model config.\n2025-07-11 15:23:29,171 - INFO - Loading pretrained ViT-B-32 weights (openai).\n2025-07-11 15:23:29,296 - INFO - Loaded ViT-L-14 model config.\n2025-07-11 15:23:31,198 - INFO - Loading pretrained ViT-L-14 weights (openai).\n2025-07-11 15:23:31,327 - INFO - Rank 0 assigned 577 out of 577 images\nMulti-model dataset created with 3 preprocessing pipelines\n</code></pre> We've loaded three different model preprocessing pipelines: two CLIP models (ViT-B-32 and ViT-L-14) with their pretrained weights, plus our custom ResNet transform. This multi-model approach allows us to process the same beetle images for different computer vision models simultaneously, which is particularly useful for comparative analysis or ensemble methods.</p> <p>For multi-model datasets, we also need to use a special collate function:</p> <pre><code>from hpc_inference.utils.distributed import multi_model_collate\n\n# Create DataLoader with multi-model collate function\nmulti_loader = DataLoader(\n    multi_model_dataset, \n    batch_size=2, \n    num_workers=1,  \n    collate_fn=multi_model_collate\n)\n\n# Get a batch and examine the output\nfor uuids, data_dict in multi_loader:\n    print(f\"Batch UUIDs: {uuids}\")\n    print(f\"Available models: {list(data_dict.keys())}\")\n\n    for model_name, tensor in data_dict.items():\n        print(f\"  {model_name}: shape {tensor.shape}, dtype {tensor.dtype}\")\n    break\n</code></pre> <pre><code>2025-07-11 15:24:23,760 - INFO - [Rank 0/Worker 0] Processing 577 images\nBatch UUIDs: ['A00000001831.jpg', 'A00000003356.jpg']\nAvailable models: ['clip_vit_b32', 'clip_vit_l14', 'resnet']\n  clip_vit_b32: shape torch.Size([2, 3, 224, 224]), dtype torch.float32\n  clip_vit_l14: shape torch.Size([2, 3, 224, 224]), dtype torch.float32\n  resnet: shape torch.Size([2, 3, 224, 224]), dtype torch.float32\n</code></pre> <p>Now we demonstrated successful multi-model batch processing where each image is automatically preprocessed for all three models simultaneously. </p> <p>The dataset returns a dictionary containing preprocessed tensors for each model, all with the same batch size (2) and image dimensions (224\u00d7224), but each potentially having different normalization and preprocessing applied according to their respective model requirements. The <code>multi_model_collate</code> function ensures proper batching of the dictionary structure across multiple samples.</p>"},{"location":"imagefolder-guide/#uuid-generation-modes","title":"UUID Generation Modes","text":"<p>The <code>ImageFolderDataset</code> supports different ways to generate unique identifiers from file paths:</p> <pre><code># Test different UUID modes\nuuid_modes = [\"filename\", \"relative\", \"fullpath\", \"hash\"]\n\nfor mode in uuid_modes:\n    print(f\"\\n--- UUID Mode: {mode} ---\")\n\n    dataset = ImageFolderDataset(\n        image_dir=image_dir,\n        preprocess=None,\n        uuid_mode=mode\n    )\n\n    loader = DataLoader(\n        dataset, \n        batch_size=2, \n        num_workers=1,\n        collate_fn=pil_image_collate  # Use custom collate for PIL images\n    )\n\n    # Get first batch and show UUIDs\n    for uuids, images in loader:\n        for uuid in uuids:\n            print(f\"  UUID: {uuid}\")\n        break\n</code></pre>"},{"location":"imagefolder-guide/#distributed-processing","title":"Distributed Processing","text":"<p>The <code>ImageFolderDataset</code> is designed for distributed processing across multiple workers/ranks. Let's simulate this by creating multiple dataset instances with different rank settings:</p> <pre><code># Simulate distributed processing with 3 workers\nworld_size = 3\nrank_datasets = []\n\nprint(f\"Distributing images across {world_size} workers:\")\n\nfor rank in range(world_size):\n    dataset = ImageFolderDataset(\n        image_dir=image_dir,\n        preprocess=None,\n        rank=rank,\n        world_size=world_size,\n        evenly_distribute=True,  # Distribute based on file sizes\n        uuid_mode=\"filename\"\n    )\n\n    rank_datasets.append(dataset)\n    print(f\"  Rank {rank}: {len(dataset)} images\")\n\n# Verify no overlap between ranks\nall_files = set()\nfor rank, dataset in enumerate(rank_datasets):\n    rank_files = set(dataset.image_files)\n    overlap = all_files.intersection(rank_files)\n    if overlap:\n        print(f\"WARNING: Rank {rank} has overlapping files: {overlap}\")\n    all_files.update(rank_files)\n\nprint(f\"\\nTotal unique files across all ranks: {len(all_files)}\")\nprint(f\"Original total files: {len(list(image_dir.glob('*.jpg')))}\")\n</code></pre> <pre><code>2025-07-11 15:27:01,756 - INFO - Rank 0 assigned 193 files (total size: 0.98 GB)\n2025-07-11 15:27:01,757 - INFO - Rank 0 assigned 193 out of 577 images\n2025-07-11 15:27:01,759 - INFO - Rank 1 assigned 192 files (total size: 0.98 GB)\n2025-07-11 15:27:01,759 - INFO - Rank 1 assigned 192 out of 577 images\n2025-07-11 15:27:01,761 - INFO - Rank 2 assigned 192 files (total size: 0.98 GB)\n2025-07-11 15:27:01,761 - INFO - Rank 2 assigned 192 out of 577 images\n2025-07-11 15:27:01,763 - INFO - [Rank 0/Worker 0] Processing 193 images\nDistributing images across 3 workers:\n  Rank 0: 193 images\n  Rank 1: 192 images\n  Rank 2: 192 images\n\nTotal unique files across all ranks: 577\nOriginal total files: 577\n\nExample from Rank 0:\n  First batch UUIDs: ['A00000051603.jpg', 'A00000051179.jpg']\n</code></pre> <p>The output demonstrates successful distributed processing where the 577 beetle images are automatically partitioned across 3 workers. </p> <p>With <code>evenly_distribute=True</code>, the dataset balances workload by file sizes (~0.98 GB per rank) rather than just file counts, ensuring more balanced processing times. Rank 0 gets 193 images while ranks 1 and 2 each get 192 images. The verification confirms no overlap between ranks and that all original files are covered.</p> <p>Learn More About File Distribution</p> <p>The file distribution logic is handled by the <code>assign_files_to_rank</code> function. For detailed implementation and additional parameters, see the API Reference and source code.</p>"},{"location":"imagefolder-guide/#visualizing-processed-images","title":"Visualizing Processed Images","text":"<p>Let's visualize some images and their processed versions to understand what the preprocessing is doing:</p> <pre><code># Create datasets for visualization\nraw_dataset = ImageFolderDataset(image_dir=image_dir, preprocess=None, uuid_mode=\"filename\")\nprocessed_dataset = ImageFolderDataset(image_dir=image_dir, preprocess=resnet_transform, uuid_mode=\"filename\")\n\n# Get loaders - note the different collate functions needed\nraw_loader = DataLoader(\n    raw_dataset, \n    batch_size=1, \n    num_workers=0,\n    collate_fn=pil_image_collate  # Custom collate for PIL images\n)\nprocessed_loader = DataLoader(\n    processed_dataset, \n    batch_size=1, \n    num_workers=0\n    # Default collate works fine for tensors\n)\n\n# Function to denormalize tensor for visualization\ndef denormalize_tensor(tensor, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]):\n    \"\"\"Denormalize a tensor for visualization.\"\"\"\n    mean = torch.tensor(mean).view(3, 1, 1)\n    std = torch.tensor(std).view(3, 1, 1)\n    return tensor * std + mean\n\n# Visualize first few images\nfig, axes = plt.subplots(2, 3, figsize=(15, 10))\nfig.suptitle('Original vs Processed Beetle Images', fontsize=16)\n\nraw_iter = iter(raw_loader)\nprocessed_iter = iter(processed_loader)\n\nfor i in range(3):\n    # Get raw image\n    uuid_raw, img_raw = next(raw_iter)\n    uuid_proc, tensor_proc = next(processed_iter)\n\n    # Display original\n    axes[0, i].imshow(img_raw[0])\n    axes[0, i].set_title(f'Original: {uuid_raw[0]}')\n    axes[0, i].axis('off')\n\n    # Display processed (denormalized)\n    denorm_tensor = denormalize_tensor(tensor_proc[0])\n    denorm_tensor = torch.clamp(denorm_tensor, 0, 1)\n    img_processed = denorm_tensor.permute(1, 2, 0).numpy()\n\n    axes[1, i].imshow(img_processed)\n    axes[1, i].set_title(f'Processed: 224x224')\n    axes[1, i].axis('off')\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p><pre><code>2025-07-11 15:34:04,113 - INFO - Rank 0 assigned 577 out of 577 images\n2025-07-11 15:34:04,114 - INFO - Rank 0 assigned 577 out of 577 images\n2025-07-11 15:34:04,179 - INFO - [Rank 0/Worker 0] Processing 577 images\n2025-07-11 15:34:04,282 - INFO - [Rank 0/Worker 0] Processing 577 images\n</code></pre> </p> <p>The top row shows the original high-resolution images (5568\u00d73712 pixels). The bottom row shows the same images after ResNet preprocessing: resized to 224\u00d7224 pixels, center-cropped, and normalized.</p>"},{"location":"imagefolder-guide/#performance-optimization","title":"Performance Optimization","text":"<p>Let's explore some performance aspects of the <code>ImageFolderDataset</code>:</p> <pre><code>import time\n\n# Test different configurations\nconfigs = [\n    {\"name\": \"Single worker\", \"num_workers\": 0, \"batch_size\": 8},\n    {\"name\": \"Multi worker\", \"num_workers\": 2, \"batch_size\": 8},\n    {\"name\": \"Larger batch\", \"num_workers\": 2, \"batch_size\": 16},\n]\n\n# Create a dataset for performance testing\nperf_dataset = ImageFolderDataset(\n    image_dir=image_dir,\n    preprocess=resnet_transform,\n    uuid_mode=\"filename\",\n    validate=False\n)\n\nprint(\"Performance comparison (processing first 50 images):\")\nprint(\"-\" * 60)\n\nfor config in configs:\n    loader = DataLoader(\n        perf_dataset,\n        batch_size=config[\"batch_size\"],\n        num_workers=config[\"num_workers\"],\n        pin_memory=True if torch.cuda.is_available() else False\n    )\n\n    start_time = time.time()\n    images_processed = 0\n\n    for uuids, tensors in loader:\n        images_processed += len(uuids)\n        if images_processed &gt;= 50:  # Stop after 50 images\n            break\n\n    elapsed = time.time() - start_time\n    images_per_sec = images_processed / elapsed\n\n    print(f\"{config['name']:15} | {images_processed:3d} images | \"\n          f\"{elapsed:5.2f}s | {images_per_sec:5.1f} img/s\")\n</code></pre> <pre><code>2025-07-11 15:35:18,793 - INFO - Rank 0 assigned 577 out of 577 images\n2025-07-11 15:35:19,738 - INFO - [Rank 0/Worker 0] Processing 577 images\nPerformance comparison (processing first 50 images):\n------------------------------------------------------------\nSingle worker   |  56 images | 14.47s |   3.9 img/s\n2025-07-11 15:35:34,238 - INFO - [Rank 0/Worker 0] Processing 289 images\n2025-07-11 15:35:34,238 - INFO - [Rank 0/Worker 1] Processing 288 images\nMulti worker    |  56 images |  7.79s |   7.2 img/s\n2025-07-11 15:35:42,006 - INFO - [Rank 0/Worker 0] Processing 289 images\n2025-07-11 15:35:42,009 - INFO - [Rank 0/Worker 1] Processing 288 images\nLarger batch    |  64 images |  8.65s |   7.4 img/s\n</code></pre> <p>The benchmark results reveal significant performance differences between configurations when processing high-resolution NEON beetle images. The single worker configuration achieved 3.9 images per second, while adding a second worker (<code>num_workers=2</code>) nearly doubled throughput to 7.2 images per second. This demonstrates that image preprocessing is I/O bound, as multiple workers can load and process images in parallel while one worker would otherwise be waiting for disk access.</p> <p>And interestingly, increasing the batch size from 8 to 16 while maintaining 2 workers showed only marginal improvement (7.4 vs 7.2 img/s), suggesting that the bottleneck shifts from I/O to processing capacity at higher batch sizes. </p> <p>Scaling with Multiple Processes</p> <p>These performance numbers above represent throughput for a single process only. In distributed processing pipelines, you can dramatically increase overall throughput by starting multiple processes in parallel (increasing <code>world_size</code>). </p> <p>Each rank gets assigned balanced files as demonstrated in the distributed processing section, allowing total throughput to scale to approximately best per-rank performance \u00d7 world_size. For example, if one process achieves 7.2 img/s, running 8 processes could theoretically reach ~57.6 img/s total throughput across the cluster.</p>"},{"location":"imagefolder-guide/#sampling-considerations","title":"Sampling Considerations","text":"<p>No Built-in Sampling Support</p> <p>The <code>ImageFolderDataset</code> is built on top of PyTorch's <code>IterableDataset</code>, which means it doesn't support built-in sampling methods like random sampling, weighted sampling, or stratified sampling. If sampling is crucial for your task (e.g., handling class imbalance, creating balanced batches, or implementing specific sampling strategies), consider using PyTorch's standard <code>Dataset</code> class instead.</p> <p>For more details on why sufficient sampling is a complex problem see this excellent talk by Nicolas Hug: Implementing and Using Iterable Datasets: What Could Go Wrong?.</p> <p>Alternative approaches for sampling with <code>ImageFolderDataset</code>:</p> <ul> <li>Create a subset of images before initializing the dataset</li> <li>Manually assign specific image subsets to different ranks</li> <li>For complex sampling needs, implement a custom <code>Dataset</code> subclass instead</li> </ul>"},{"location":"imagefolder-guide/#best-practices-and-tips","title":"Best Practices and Tips","text":"<p>Based on the examples above, here are key recommendations for using <code>ImageFolderDataset</code> effectively:</p>"},{"location":"imagefolder-guide/#choose-the-right-uuid-mode","title":"Choose the Right UUID Mode","text":"<ul> <li>Use <code>\"filename\"</code> for simple cases where filenames are unique</li> <li>Use <code>\"relative\"</code> when you have subdirectories and need to preserve path structure</li> <li>Use <code>\"hash\"</code> for anonymization or when dealing with non-unique filenames across nested folders</li> </ul>"},{"location":"imagefolder-guide/#validation","title":"Validation","text":"<ul> <li>Enable <code>validate=True</code> when working with unknown image quality</li> <li>Disable it for trusted datasets to improve performance</li> <li>Validate image folder with PIL in advance to reduce GPU waiting time</li> </ul>"},{"location":"imagefolder-guide/#multi-model-processing","title":"Multi-Model Processing","text":"<ul> <li>Use multi-model preprocessing when you need different input formats</li> <li>Remember to use the <code>multi_model_collate</code> function in your DataLoader</li> </ul>"},{"location":"imagefolder-guide/#distributed-processing-performance-optimization","title":"Distributed Processing &amp; Performance Optimization","text":"<ul> <li>Set <code>evenly_distribute=True</code> for better load balancing across workers</li> <li>Increase <code>num_workers</code> for I/O bound tasks</li> <li>Use larger batch sizes when memory allows</li> <li>Enable <code>pin_memory=True</code> when using GPU</li> </ul>"},{"location":"imagefolder-guide/#collate-functions","title":"Collate Functions","text":"<ul> <li>Use <code>pil_image_collate</code> when <code>preprocess=None</code> (returning PIL Images)</li> <li>Use <code>multi_model_collate</code> for multi-model preprocessing</li> <li>Use default collate function for single tensor outputs</li> </ul>"},{"location":"utilization-metrics/","title":"GPU Utilization Metrics","text":""},{"location":"utilization-metrics/#key-desiderata-for-a-gpu-utilization-statistic","title":"Key Desiderata for a GPU Utilization Statistic","text":"<p>An ideal metric for GPU utilization should reward high average usage, penalize low/idle time, and account for stability (low variance).</p> <ol> <li>High mean utilization is good.</li> <li>Low variance (stable, not spiky) is good.</li> <li>Duration/time-normalization is important\u2014long idle periods should penalize the score.</li> <li>Interpretable on [0, 1] or [0%, 100%] scale if possible.</li> </ol>"},{"location":"utilization-metrics/#basic-statistics","title":"Basic Statistics","text":"<p>Let \\(u_1, u_2, ..., u_n\\) be the sequence of GPU utilization percentages (sampled at regular intervals, in [0, 100]).</p>"},{"location":"utilization-metrics/#a-mean-utilization","title":"A. Mean Utilization","text":"\\[ \\mu_u = \\frac{1}{n} \\sum_{i=1}^n u_i \\] <ul> <li>Pros: Simple, intuitive.</li> <li>Cons: Can be misleading if you have brief spikes and lots of idle periods.</li> </ul>"},{"location":"utilization-metrics/#b-standard-deviation-variance","title":"B. Standard Deviation (Variance)","text":"\\[ \\sigma_u = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n (u_i - \\mu_u)^2 } \\] <ul> <li>High \u03c3: Utilization is unstable/spiky.</li> </ul>"},{"location":"utilization-metrics/#c-proposed-robust-utilization-metric","title":"C. Proposed Robust Utilization Metric","text":""},{"location":"utilization-metrics/#1-effective-utilization-mean-std","title":"1. \u201cEffective Utilization\u201d (Mean - \u03bb \u00d7 Std)","text":"\\[ \\text{EffU} = \\mu_u - \\lambda \\sigma_u \\] <ul> <li>Where \u03bb is a tradeoff factor (e.g., \u03bb=1).</li> <li>Interpretation: Rewards high mean, penalizes high variance.</li> </ul>"},{"location":"utilization-metrics/#2-fraction-of-time-above-threshold","title":"2. Fraction of Time Above Threshold","text":"\\[ \\text{Frac}_{\\theta} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{ u_i &gt; \\theta \\} \\] <ul> <li>e.g., \u03b8=80%.</li> <li>What fraction of the run is \u201chighly utilized\u201d?</li> </ul>"},{"location":"utilization-metrics/#3-area-under-the-utilization-curve-auc","title":"3. Area Under the Utilization Curve (AUC)","text":"\\[ \\text{AUC}_u = \\frac{1}{100 n} \\sum_{i=1}^n u_i \\] <ul> <li>Same as mean, but normalized to [0,1].</li> <li>AUC is also robust if your sampling interval is uniform.</li> <li>AUC only reflects \u201caverage work done,\u201d not how the work was distributed in time. For hardware optimization and system diagnosis, you also want to know if the workload is steady or bursty, and how often the GPU is left waiting.</li> </ul>"},{"location":"utilization-metrics/#d-composite-gpu-efficiency-score","title":"D. Composite \u201cGPU Efficiency Score\u201d","text":"<p>Let\u2019s define a simple composite metric:</p> \\[ \\text{GPU Efficiency} = \\frac{\\text{Mean Util} - \\sigma_u}{100} \\] <ul> <li>Range: Can be negative (bad) or up to 1 (perfect: mean=100, std=0).</li> <li> <p>Interpretation:</p> </li> <li> <p>1.0: Always 100%, perfectly steady.</p> </li> <li>0.8: Average 90%, std 10.</li> <li>Negative: mean is low and/or std is very high (spiky/idle).</li> </ul> <p>Or:</p> \\[ \\text{GPU Utilization Score} = \\frac{1}{100} \\left( \\alpha \\cdot \\mu_u + (1 - \\alpha) \\cdot \\text{Frac}_{\\theta} \\right) \\] <p>Where \u03b1 is a weight (e.g., 0.5), \u03b8 is a high-utilization threshold (e.g., 80%).</p>"},{"location":"utilization-metrics/#e-time-weighted-adjustment-if-needed","title":"E. Time-Weighted Adjustment (if needed)","text":"<p>If intervals are not uniform, multiply each utilization by its interval and divide by total time:</p> \\[ \\text{TimeWeightedMean} = \\frac{ \\sum_{i=1}^n u_i \\Delta t_i }{ \\sum_{i=1}^n \\Delta t_i } \\]"},{"location":"utilization-metrics/#critiquelimitations","title":"Critique/Limitations","text":"<ul> <li>High mean but high variance may indicate batchiness, pipeline stalling\u2014lower score with the above metric.</li> <li>High mean with low std is truly optimal (score near 1).</li> <li>Low mean and low std means consistently idle (score near 0 or negative).</li> <li>Composite metrics can be tuned (\u03bb or \u03b1) to emphasize stability or average, depending on workload.</li> </ul>"},{"location":"utilization_stat/","title":"Utilization stat","text":""},{"location":"utilization_stat/#key-desiderata-for-a-gpu-utilization-statistic","title":"Key Desiderata for a GPU Utilization Statistic","text":"<p>An ideal metric for GPU utilization should reward high average usage, penalize low/idle time, and account for stability (low variance).</p> <ol> <li>High mean utilization is good.</li> <li>Low variance (stable, not spiky) is good.</li> <li>Duration/time-normalization is important\u2014long idle periods should penalize the score.</li> <li>Interpretable on [0, 1] or [0%, 100%] scale if possible.</li> </ol>"},{"location":"utilization_stat/#basic-statistics","title":"Basic Statistics","text":"<p>Let \\(u_1, u_2, ..., u_n\\) be the sequence of GPU utilization percentages (sampled at regular intervals, in [0, 100]).</p>"},{"location":"utilization_stat/#a-mean-utilization","title":"A. Mean Utilization","text":"\\[ \\mu_u = \\frac{1}{n} \\sum_{i=1}^n u_i \\] <ul> <li>Pros: Simple, intuitive.</li> <li>Cons: Can be misleading if you have brief spikes and lots of idle periods.</li> </ul>"},{"location":"utilization_stat/#b-standard-deviation-variance","title":"B. Standard Deviation (Variance)","text":"\\[ \\sigma_u = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n (u_i - \\mu_u)^2 } \\] <ul> <li>High \u03c3: Utilization is unstable/spiky.</li> </ul>"},{"location":"utilization_stat/#c-proposed-robust-utilization-metric","title":"C. Proposed Robust Utilization Metric","text":""},{"location":"utilization_stat/#1-effective-utilization-mean-std","title":"1. \u201cEffective Utilization\u201d (Mean - \u03bb \u00d7 Std)","text":"\\[ \\text{EffU} = \\mu_u - \\lambda \\sigma_u \\] <ul> <li>Where \u03bb is a tradeoff factor (e.g., \u03bb=1).</li> <li>Interpretation: Rewards high mean, penalizes high variance.</li> </ul>"},{"location":"utilization_stat/#2-fraction-of-time-above-threshold","title":"2. Fraction of Time Above Threshold","text":"\\[ \\text{Frac}_{\\theta} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{ u_i &gt; \\theta \\} \\] <ul> <li>e.g., \u03b8=80%.</li> <li>What fraction of the run is \u201chighly utilized\u201d?</li> </ul>"},{"location":"utilization_stat/#3-area-under-the-utilization-curve-auc","title":"3. Area Under the Utilization Curve (AUC)","text":"\\[ \\text{AUC}_u = \\frac{1}{100 n} \\sum_{i=1}^n u_i \\] <ul> <li>Same as mean, but normalized to [0,1].</li> <li>AUC is also robust if your sampling interval is uniform.</li> <li>AUC only reflects \u201caverage work done,\u201d not how the work was distributed in time. For hardware optimization and system diagnosis, you also want to know if the workload is steady or bursty, and how often the GPU is left waiting.</li> </ul>"},{"location":"utilization_stat/#d-composite-gpu-efficiency-score","title":"D. Composite \u201cGPU Efficiency Score\u201d","text":"<p>Let\u2019s define a simple composite metric:</p> \\[ \\text{GPU Efficiency} = \\frac{\\text{Mean Util} - \\sigma_u}{100} \\] <ul> <li>Range: Can be negative (bad) or up to 1 (perfect: mean=100, std=0).</li> <li> <p>Interpretation:</p> </li> <li> <p>1.0: Always 100%, perfectly steady.</p> </li> <li>0.8: Average 90%, std 10.</li> <li>Negative: mean is low and/or std is very high (spiky/idle).</li> </ul> <p>Or:</p> \\[ \\text{GPU Utilization Score} = \\frac{1}{100} \\left( \\alpha \\cdot \\mu_u + (1 - \\alpha) \\cdot \\text{Frac}_{\\theta} \\right) \\] <p>Where \u03b1 is a weight (e.g., 0.5), \u03b8 is a high-utilization threshold (e.g., 80%).</p>"},{"location":"utilization_stat/#e-time-weighted-adjustment-if-needed","title":"E. Time-Weighted Adjustment (if needed)","text":"<p>If intervals are not uniform, multiply each utilization by its interval and divide by total time:</p> \\[ \\text{TimeWeightedMean} = \\frac{ \\sum_{i=1}^n u_i \\Delta t_i }{ \\sum_{i=1}^n \\Delta t_i } \\]"},{"location":"utilization_stat/#f-example-in-python","title":"F. Example in Python","text":"<pre><code>import numpy as np\n\ngpu_util = np.array([...])  # Your utilization sequence (0-100)\nmean_util = gpu_util.mean()\nstd_util = gpu_util.std()\nfrac_high = np.mean(gpu_util &gt; 80)\ngpu_efficiency = (mean_util - std_util) / 100\nauc = gpu_util.mean() / 100\n\nprint(f\"Mean Util: {mean_util:.1f}%\")\nprint(f\"Std Util: {std_util:.1f}\")\nprint(f\"Fraction &gt; 80%: {frac_high:.2f}\")\nprint(f\"GPU Efficiency: {gpu_efficiency:.3f}\")\nprint(f\"AUC (normalized): {auc:.3f}\")\n</code></pre>"},{"location":"utilization_stat/#critiquelimitations","title":"Critique/Limitations","text":"<ul> <li>High mean but high variance may indicate batchiness, pipeline stalling\u2014lower score with the above metric.</li> <li>High mean with low std is truly optimal (score near 1).</li> <li>Low mean and low std means consistently idle (score near 0 or negative).</li> <li>Composite metrics can be tuned (\u03bb or \u03b1) to emphasize stability or average, depending on workload.</li> </ul>"}]}