{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"HPC-Inference","text":"HPC-Inference <p>Batch inference solution for large-scale image datasets on HPC</p>"},{"location":"#about","title":"About","text":"<p>Problem: Many batch inference workflows waste GPU resources due to I/O bottlenecks and sequential processing, leading to poor GPU utilization and longer processing times.</p>"},{"location":"#key-bottlenecks","title":"Key Bottlenecks","text":"<ul> <li>Slow sequential large file loading (Disk \u2192 RAM)</li> <li>Single-threaded image preprocessing</li> <li>Data transfer delays (CPU \u2194 GPU)</li> <li>GPU idle time waiting for data</li> <li>Sequential output writing</li> </ul>"},{"location":"#hpc-inference-solutions","title":"HPC-Inference Solutions","text":"<ul> <li>Parallel data loading: Eliminates disk I/O bottlenecks with optimized dataset loaders</li> <li>Asynchronous preprocessing: Keeps GPUs fed with continuous data queues  </li> <li>SLURM integration: Deploy seamlessly on HPC clusters</li> <li>Multi-GPU distribution: Scales across HPC nodes for maximum throughput</li> <li>Resource profiling: Logs timing metrics and CPU/GPU usage rates to help optimize your configuration</li> </ul>"},{"location":"#core-features","title":"Core Features","text":"<p>The <code>hpc_inference</code> package's core functionality includes customized PyTorch datasets:</p> <ul> <li><code>ParquetImageDataset</code> for image data stored as compressed binary columns across multiple large Parquet files</li> <li><code>ImageFolderDataset</code> for image data stored in folders using open file formats such as <code>JPG</code>, <code>PNG</code>, <code>TIFF</code>, etc.</li> </ul> <p>The package also comes with a suite of ready-to-use job scripts to perform efficient batch inference using pretrained models on HPCs.</p>"},{"location":"#use-cases","title":"Use Cases","text":"<ol> <li>Image Folder Dataset - Process images from directory structures</li> <li>Parquet Dataset - Handle compressed image data in Parquet format</li> <li>Large scale CLIP embedding - Generate embeddings for massive datasets</li> <li>Large scale face detection - Detect faces across large image collections</li> <li>Large scale animal detection - Use MegaDetector for wildlife analysis</li> <li>Grid search profiling - Optimize processing parameters</li> </ol>"},{"location":"#quick-links","title":"Quick Links","text":""},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>This project is a joint effort between the Imageomics Institute and the ABC Global Center.</p>"},{"location":"api-reference/","title":"API Reference","text":"<p>This page provides comprehensive API documentation for the HPC-Inference package.</p>"},{"location":"api-reference/#hpc_inference.datasets","title":"<code>hpc_inference.datasets</code>","text":"<p>High-performance dataset loaders for image data.</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset","title":"<code>ImageFolderDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Loads images from a folder in a streaming fashion, with support for distributed processing. - Handles common image extensions - Loads images as RGB by default (can be changed via color_mode) - Validates images using PIL if validate=True - Supports distributed processing with rank-based file partitioning - Supports multi-worker data loading within each rank</p> <p>Returns (uuid, processed_data) tuples where: - uuid: filename (without path) as identifier - processed_data: tensor (single model) or dict of tensors (multi-model)</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.__init__","title":"<code>__init__(image_dir, preprocess=None, color_mode='RGB', validate=False, rank=None, world_size=None, evenly_distribute=True, stagger=False, uuid_mode='filename')</code>","text":"<p>Parameters:</p> Name Type Description Default <code>image_dir</code> <code>Union[str, Path]</code> <p>Path to image folder.</p> required <code>preprocess</code> <code>Optional[Union[Callable, Dict[str, Callable]]]</code> <p>Transform(s) to apply to images. - If callable: single model preprocessing - If dict: {model_name: preprocess_fn} for multi-model - If None: return PIL image as-is</p> <code>None</code> <code>color_mode</code> <code>str</code> <p>Color mode for PIL.Image.convert (default: \"RGB\").</p> <code>'RGB'</code> <code>validate</code> <code>bool</code> <p>If True, validates images in the directory using PIL.</p> <code>False</code> <code>rank</code> <code>Optional[int]</code> <p>Current process rank for distributed processing.</p> <code>None</code> <code>world_size</code> <code>Optional[int]</code> <p>Total number of processes for distributed processing.</p> <code>None</code> <code>evenly_distribute</code> <code>bool</code> <p>Whether to distribute files evenly based on size. Defaults to True. If False, files are distributed in a round-robin manner.</p> <code>True</code> <code>stagger</code> <code>bool</code> <p>Whether to stagger the start of each worker. Defaults to False.</p> <code>False</code> <code>uuid_mode</code> <code>Literal['filename', 'relative', 'fullpath', 'hash']</code> <p>How to generate UUIDs from image paths. - \"filename\": Use just the filename (image001.jpg) - \"relative\": Use relative path from image_dir (subfolder/image001.jpg) - \"fullpath\": Use full absolute path - \"hash\": Use hash of the full path</p> <code>'filename'</code>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over images, yielding (uuid, processed_image) tuples. Supports multi-worker data loading within each rank.</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.__len__","title":"<code>__len__()</code>","text":"<p>Return approximate length (actual length depends on worker assignment).</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.parse_image","title":"<code>parse_image(img_path)</code>","text":"<p>Load and process a single image.</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.validate_PIL","title":"<code>validate_PIL(file_path)</code>  <code>staticmethod</code>","text":"<p>Validates if the file can be opened by PIL. Returns True if valid, False otherwise.</p>"},{"location":"api-reference/#hpc_inference.datasets.ImageFolderDataset.validate_image_files","title":"<code>validate_image_files(image_files, max_workers=16)</code>  <code>classmethod</code>","text":"<p>Validates a list of image files using PIL.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetEmbeddingDataset","title":"<code>ParquetEmbeddingDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Loads pre-computed embeddings from Parquet files in a streaming fashion. - Reads embedding vectors from Parquet files - Supports distributed processing with rank-based file partitioning - Supports multi-worker data loading within each rank - Optimized for loading numerical data (embeddings) rather than images</p> <p>Returns (uuid, embedding) tuples where: - uuid: Unique identifier from the UUID column in Parquet - embedding: Numpy array or tensor containing the embedding vector</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetEmbeddingDataset.__init__","title":"<code>__init__(parquet_files, col_uuid='uuid', col_embedding='embedding', rank=None, world_size=None, evenly_distribute=True, read_batch_size=1000, read_columns=None, stagger=False, return_tensor=True)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>parquet_files</code> <code>List[Union[str, Path]]</code> <p>List of paths to Parquet files containing embedding data.</p> required <code>col_uuid</code> <code>str</code> <p>Name of the UUID column in Parquet files. Defaults to \"uuid\".</p> <code>'uuid'</code> <code>col_embedding</code> <code>str</code> <p>Name of the embedding column in Parquet files. Defaults to \"embedding\".</p> <code>'embedding'</code> <code>rank</code> <code>Optional[int]</code> <p>Current process rank for distributed processing.</p> <code>None</code> <code>world_size</code> <code>Optional[int]</code> <p>Total number of processes for distributed processing.</p> <code>None</code> <code>evenly_distribute</code> <code>bool</code> <p>Whether to distribute files evenly based on size. Defaults to True. If False, files are distributed in a round-robin manner.</p> <code>True</code> <code>read_batch_size</code> <code>int</code> <p>Number of rows to read from Parquet at a time. Defaults to 1000.</p> <code>1000</code> <code>read_columns</code> <code>Optional[List[str]]</code> <p>List of column names to read from Parquet. If None, reads all columns. Typically includes [\"uuid\", \"embedding\"].</p> <code>None</code> <code>stagger</code> <code>bool</code> <p>Whether to stagger the start of each worker. Defaults to False.</p> <code>False</code> <code>return_tensor</code> <code>bool</code> <p>If True, convert embeddings to PyTorch tensors. If False, keep as numpy arrays.</p> <code>True</code>"},{"location":"api-reference/#hpc_inference.datasets.ParquetEmbeddingDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over Parquet files and yield embedding data. Supports multi-worker data loading within each rank.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetEmbeddingDataset.process_parquet_file","title":"<code>process_parquet_file(file_path)</code>","text":"<p>Process a single Parquet file and yield embedding data.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the Parquet file to process.</p> required <p>Yields:</p> Type Description <code>Tuple[str, Any]</code> <p>Tuples of (uuid, embedding) for each row in the file.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset","title":"<code>ParquetImageDataset</code>","text":"<p>               Bases: <code>IterableDataset</code></p> <p>Loads images from Parquet files in a streaming fashion, with support for distributed processing. - Reads image data from Parquet files containing encoded image bytes - Supports distributed processing with rank-based file partitioning - Supports multi-worker data loading within each rank - Handles both single-model and multi-model preprocessing - Provides staggered worker starts and load balancing</p> <p>Returns (uuid, processed_data) tuples where: - uuid: Unique identifier from the UUID column in Parquet - processed_data: Tensor (single model) or dict of tensors (multi-model)</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset.__init__","title":"<code>__init__(parquet_files, col_uuid='uuid', rank=None, world_size=None, evenly_distribute=True, decode_fn=None, preprocess=None, read_batch_size=128, read_columns=None, stagger=False, processed_files_log=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>parquet_files</code> <code>List[Union[str, Path]]</code> <p>List of paths to Parquet files containing image data.</p> required <code>col_uuid</code> <code>str</code> <p>Name of the UUID column in Parquet files. Defaults to \"uuid\".</p> <code>'uuid'</code> <code>rank</code> <code>Optional[int]</code> <p>Current process rank for distributed processing.</p> <code>None</code> <code>world_size</code> <code>Optional[int]</code> <p>Total number of processes for distributed processing.</p> <code>None</code> <code>evenly_distribute</code> <code>bool</code> <p>Whether to distribute files evenly based on size. Defaults to True. If False, files are distributed in a round-robin manner.</p> <code>True</code> <code>decode_fn</code> <code>Optional[Callable]</code> <p>Function to decode image bytes to PIL Image. Required for image processing.</p> <code>None</code> <code>preprocess</code> <code>Optional[Union[Callable, Dict[str, Callable]]]</code> <p>Transform(s) to apply to images. - If callable: single model preprocessing - If dict: {model_name: preprocess_fn} for multi-model - If None: return decoded image as-is</p> <code>None</code> <code>read_batch_size</code> <code>int</code> <p>Number of rows to read from Parquet at a time. Defaults to 128.</p> <code>128</code> <code>read_columns</code> <code>Optional[List[str]]</code> <p>List of column names to read from Parquet. If None, reads all columns. Typically includes [\"uuid\", \"image\", \"original_size\", \"resized_size\"].</p> <code>None</code> <code>stagger</code> <code>bool</code> <p>Whether to stagger the start of each worker. Defaults to False.</p> <code>False</code> <code>processed_files_log</code> <code>Optional[Union[str, Path]]</code> <p>Path to log file for tracking processed files. Optional.</p> <code>None</code>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset.__iter__","title":"<code>__iter__()</code>","text":"<p>Iterate over Parquet files and yield processed data. Supports multi-worker data loading within each rank.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset.log_processed_file","title":"<code>log_processed_file(file_path)</code>","text":"<p>Log a processed file to the log file if configured.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset.parse_batch_data","title":"<code>parse_batch_data(batch_df)</code>","text":"<p>Parse a batch of data from Parquet DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>batch_df</code> <code>DataFrame</code> <p>DataFrame containing batch data from Parquet file.</p> required <p>Yields:</p> Type Description <code>Tuple[str, Any]</code> <p>Tuples of (uuid, processed_data) for each row in the batch.</p>"},{"location":"api-reference/#hpc_inference.datasets.ParquetImageDataset.process_parquet_file","title":"<code>process_parquet_file(file_path)</code>","text":"<p>Process a single Parquet file and yield processed data.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the Parquet file to process.</p> required <p>Yields:</p> Type Description <code>Tuple[str, Any]</code> <p>Tuples of (uuid, processed_data) for each valid row in the file.</p>"},{"location":"api-reference/#hpc_inference.datasets.multi_model_collate","title":"<code>multi_model_collate(batch)</code>","text":"<p>Collate function for batches where each sample is (uuid, {model_name: tensor, ...}).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tuple[str, Dict[str, Any]]]</code> <p>List of (uuid, processed_data_dict) tuples from dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], Dict[str, Tensor]]</code> <p>Tuple containing: - uuids: List of UUID strings - batch_dict: Dict mapping model names to batched tensors</p>"},{"location":"api-reference/#hpc_inference.inference","title":"<code>hpc_inference.inference</code>","text":"<p>Inference modules for various model types.</p>"},{"location":"api-reference/#hpc_inference.utils","title":"<code>hpc_inference.utils</code>","text":"<p>Utility functions for HPC inference.</p>"},{"location":"api-reference/#hpc_inference.utils.assign_files_to_rank","title":"<code>assign_files_to_rank(rank, world_size, files, evenly_distribute=True)</code>","text":"<p>Assign files to the current rank based on the world size.</p> <p>This method ensures that each rank gets a unique set of files to process. The files can be distributed evenly based on their size (LPT algorithm) or simply by their order. This is useful for large datasets where some files may be significantly larger than others.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Current process rank (0-indexed).</p> required <code>world_size</code> <code>int</code> <p>Total number of processes across all ranks.</p> required <code>files</code> <code>List[Union[str, Path]]</code> <p>List of file paths to distribute across ranks.</p> required <code>evenly_distribute</code> <code>bool</code> <p>Whether to distribute files evenly based on file size. Defaults to True. - If True: Uses Longest Processing Time (LPT) algorithm for load balancing - If False: Uses simple round-robin distribution</p> <code>True</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List of file paths assigned to the given rank.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If rank is negative or &gt;= world_size.</p> <code>FileNotFoundError</code> <p>If any file in the list doesn't exist when evenly_distribute=True.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; files = [\"/path/file1.parquet\", \"/path/file2.parquet\", \"/path/file3.parquet\"]\n&gt;&gt;&gt; assign_files_to_rank(0, 2, files, evenly_distribute=False)\n[\"/path/file1.parquet\", \"/path/file3.parquet\"]\n</code></pre> <pre><code>&gt;&gt;&gt; assign_files_to_rank(1, 2, files, evenly_distribute=False) \n[\"/path/file2.parquet\"]\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.assign_indices_to_rank","title":"<code>assign_indices_to_rank(rank, world_size, total_items, evenly_distribute=True)</code>","text":"<p>Assign item indices to the current rank based on the world size.</p> <p>This function calculates which range of indices each rank should process when working with datasets that can be split by index ranges.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Current process rank (0-indexed).</p> required <code>world_size</code> <code>int</code> <p>Total number of processes across all ranks.</p> required <code>total_items</code> <code>int</code> <p>Total number of items to distribute across ranks.</p> required <code>evenly_distribute</code> <code>bool</code> <p>Whether to distribute items evenly. Defaults to True. - If True: Each rank gets approximately equal number of items - If False: Simple round-robin style distribution</p> <code>True</code> <p>Returns:</p> Type Description <code>int</code> <p>Tuple of (start_idx, end_idx) representing the range of indices for this rank.</p> <code>int</code> <p>The range is [start_idx, end_idx) (end_idx is exclusive).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If rank is negative or &gt;= world_size, or if total_items is negative.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; assign_indices_to_rank(0, 3, 10, evenly_distribute=True)\n(0, 4)  # Rank 0 gets indices 0,1,2,3\n</code></pre> <pre><code>&gt;&gt;&gt; assign_indices_to_rank(1, 3, 10, evenly_distribute=True) \n(4, 7)  # Rank 1 gets indices 4,5,6\n</code></pre> <pre><code>&gt;&gt;&gt; assign_indices_to_rank(2, 3, 10, evenly_distribute=True)\n(7, 10) # Rank 2 gets indices 7,8,9\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.decode_image","title":"<code>decode_image(row_data, return_type='pil')</code>","text":"<p>Decode an image stored as raw bytes in a row dictionary into a NumPy array or PIL Image.</p> <p>The function expects the row dictionary to contain an \"image\" key with raw image bytes, and either an \"original_size\" or \"resized_size\" key specifying the (height, width) of the image.</p> <p>It assumes the image has RGB channels, and reshapes the byte buffer accordingly.</p> <p>Parameters:</p> Name Type Description Default <code>row_data</code> <code>Union[Dict[str, Any], Series]</code> <p>Dictionary containing image data and metadata. - \"image\": Raw image bytes (bytes or bytearray). - \"original_size\" or \"resized_size\": Tuple (height, width) of the image.</p> required <code>return_type</code> <code>Literal['numpy', 'pil']</code> <p>Type of the returned image. Can be \"numpy\" for NumPy array or \"pil\" for PIL Image.</p> <code>'pil'</code> <p>Returns:</p> Type Description <code>Optional[Union[ndarray, Image]]</code> <p>Decoded image as a NumPy array (RGB format) or PIL Image, or None if decoding fails.</p> <p>Raises:</p> Type Description <code>None</code> <p>Returns None on any decoding error, with warning logged.</p> Notes <ul> <li>Returns None if the image size does not match the expected dimensions.</li> <li>Logs a warning if decoding fails.</li> <li>Converts BGR to RGB channel order automatically.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; row = {\"image\": image_bytes, \"original_size\": (224, 224)}\n&gt;&gt;&gt; img = decode_image(row, return_type=\"pil\")\n&gt;&gt;&gt; isinstance(img, Image.Image)\nTrue\n</code></pre> <pre><code>&gt;&gt;&gt; img_array = decode_image(row, return_type=\"numpy\")\n&gt;&gt;&gt; img_array.shape\n(224, 224, 3)\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.format_time","title":"<code>format_time(seconds)</code>","text":"<p>Format seconds into a human-readable time string.</p> <p>Parameters:</p> Name Type Description Default <code>seconds</code> <code>float</code> <p>Time duration in seconds.</p> required <p>Returns:</p> Type Description <code>str</code> <p>Formatted time string (e.g., \"1h 30m 45.67s\", \"2m 15.34s\", \"12.45s\").</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; format_time(3661.5)\n'1h 1m 1.50s'\n&gt;&gt;&gt; format_time(125.67)\n'2m 5.67s'\n&gt;&gt;&gt; format_time(45.23)\n'45.23s'\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.get_distributed_info","title":"<code>get_distributed_info()</code>","text":"<p>Get distributed training information from environment variables.</p> <p>This function extracts rank and world size from common distributed training environment variables (SLURM, torchrun, etc.).</p> <p>Returns:</p> Type Description <code>int</code> <p>Tuple of (rank, world_size) extracted from environment variables.</p> <code>int</code> <p>If no distributed environment is detected, returns (0, 1).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # In SLURM environment\n&gt;&gt;&gt; get_distributed_info()\n(2, 8)  # rank=2, world_size=8\n</code></pre> <pre><code>&gt;&gt;&gt; # In single process environment  \n&gt;&gt;&gt; get_distributed_info()\n(0, 1)  # rank=0, world_size=1\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.load_config","title":"<code>load_config(config_path)</code>","text":"<p>Load YAML configuration file.</p> <p>Parameters:</p> Name Type Description Default <code>config_path</code> <code>Union[str, Path]</code> <p>Path to YAML configuration file.</p> required <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Parsed configuration as a dictionary.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the configuration file doesn't exist.</p> <code>YAMLError</code> <p>If the YAML file is malformed.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; config = load_config(\"config.yaml\")\n&gt;&gt;&gt; print(config[\"batch_size\"])\n32\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.multi_model_collate","title":"<code>multi_model_collate(batch)</code>","text":"<p>Collate function for batches where each sample is (uuid, {model_name: tensor, ...}).</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tuple[str, Dict[str, Any]]]</code> <p>List of (uuid, processed_data_dict) tuples from dataset.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], Dict[str, Tensor]]</code> <p>Tuple containing: - uuids: List of UUID strings - batch_dict: Dict mapping model names to batched tensors</p>"},{"location":"api-reference/#hpc_inference.utils.pil_image_collate","title":"<code>pil_image_collate(batch)</code>","text":"<p>Custom collate function for batches containing PIL Images.</p> <p>This function is required when working with datasets that return PIL Images because PyTorch's default collate function only handles tensors, numpy arrays, numbers, dicts, and lists - not PIL Image objects.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>List[Tuple[str, Any]]</code> <p>List of (uuid, image) tuples where image is a PIL Image.    Each tuple contains a UUID string and a PIL Image object.</p> required <p>Returns:</p> Type Description <code>Tuple[List[str], List[Any]]</code> <p>Tuple containing: - uuids: List of UUID strings from the batch - images: List of PIL Image objects from the batch</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from PIL import Image\n&gt;&gt;&gt; batch = [(\"img1.jpg\", Image.new(\"RGB\", (100, 100))), \n...           (\"img2.jpg\", Image.new(\"RGB\", (200, 200)))]\n&gt;&gt;&gt; uuids, images = pil_image_collate(batch)\n&gt;&gt;&gt; print(uuids)\n['img1.jpg', 'img2.jpg']\n&gt;&gt;&gt; print([img.size for img in images])\n[(100, 100), (200, 200)]\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.save_emb_to_parquet","title":"<code>save_emb_to_parquet(uuids, embeddings, path, compression='zstd')</code>","text":"<p>Save embeddings (as a dict of column names to tensors/arrays) and uuids to a Parquet file.</p> <p>Parameters:</p> Name Type Description Default <code>uuids</code> <code>List[str]</code> <p>List of unique identifiers corresponding to each embedding.</p> required <code>embeddings</code> <code>Dict[str, Union[Tensor, ndarray]]</code> <p>Dictionary mapping column names to torch.Tensor or np.ndarray. Each tensor/array should have the same first dimension as len(uuids).</p> required <code>path</code> <code>Union[str, Path]</code> <p>Output Parquet file path.</p> required <code>compression</code> <code>str</code> <p>Compression type for Parquet file. Defaults to \"zstd\". Other options: \"snappy\", \"gzip\", \"brotli\", \"lz4\", \"uncompressed\".</p> <code>'zstd'</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If embeddings have mismatched dimensions or empty inputs.</p> <code>IOError</code> <p>If file cannot be written.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; uuids = [\"img_001\", \"img_002\", \"img_003\"]\n&gt;&gt;&gt; embeddings = {\n...     \"clip_emb\": torch.randn(3, 512),\n...     \"resnet_emb\": torch.randn(3, 2048)\n... }\n&gt;&gt;&gt; save_emb_to_parquet(uuids, embeddings, \"embeddings.parquet\")\n</code></pre>"},{"location":"api-reference/#hpc_inference.utils.validate_distributed_setup","title":"<code>validate_distributed_setup(rank, world_size)</code>","text":"<p>Validate distributed training setup parameters.</p> <p>Parameters:</p> Name Type Description Default <code>rank</code> <code>int</code> <p>Process rank to validate.</p> required <code>world_size</code> <code>int</code> <p>World size to validate.</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if setup is valid, False otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parameters are invalid.</p>"},{"location":"api-reference/#quick-reference","title":"Quick Reference","text":""},{"location":"api-reference/#core-classes","title":"Core Classes","text":"<ul> <li><code>ImageFolderDataset</code> - Process images from directory structures</li> <li><code>ParquetImageDataset</code> - Handle compressed image data in Parquet format</li> </ul>"},{"location":"api-reference/#utility-functions","title":"Utility Functions","text":"<ul> <li>GPU utilization monitoring functions</li> <li>Performance profiling utilities</li> <li>SLURM integration helpers</li> </ul>"},{"location":"api-reference/#configuration","title":"Configuration","text":"<ul> <li>Model configuration templates</li> <li>Preprocessing pipelines</li> <li>Batch processing optimization</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":""},{"location":"getting-started/#setup-with-uv","title":"Setup with uv","text":"<pre><code># Clone Repository\ngit clone https://github.com/Imageomics/hpc-inference.git\ncd hpc-inference\n\n# Install uv (if not already installed)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment and install package\nuv venv hpc-inference-env\nsource hpc-inference-env/bin/activate\n\n# Install base package\nuv pip install -e .\n</code></pre>"},{"location":"getting-started/#verify-installation","title":"Verify Installation","text":"<pre><code># Test the installation\npython -c \"import hpc_inference; print('\u2713 HPC-Inference installed successfully')\"\n</code></pre>"},{"location":"getting-started/#optional-dependencies","title":"Optional Dependencies","text":"<p>The package comes with ready-to-use job scripts for various use cases. Install additional dependencies based on your needs:</p> <pre><code># Check installation status and available features\npython -c \"from hpc_inference import print_installation_guide; print_installation_guide()\"\n\n# Install specific feature sets\nuv pip install -e \".[openclip]\"     # For CLIP embedding\nuv pip install -e \".[detection]\"    # For face/animal detection  \nuv pip install -e \".[all]\"          # Install dependency for all use cases\n</code></pre>"},{"location":"getting-started/#quick-start","title":"Quick Start","text":""},{"location":"getting-started/#basic-usage","title":"Basic Usage","text":""},{"location":"getting-started/#next-steps","title":"Next Steps","text":"<ul> <li>Explore the API Reference for detailed documentation</li> </ul>"},{"location":"imagefolder-guide/","title":"ImageFolder Dataset Guide","text":"<p>This guide provides a comprehensive tutorial on using the <code>ImageFolderDataset</code> class from the HPC-Inference package.</p>"},{"location":"imagefolder-guide/#overview","title":"Overview","text":"<p>The <code>ImageFolderDataset</code> is designed for efficient processing of image datasets stored in folder structures. It provides optimized data loading capabilities for HPC environments with features like:</p> <ul> <li>Parallel data loading</li> <li>Memory-efficient processing</li> <li>Integration with PyTorch <code>DataLoader</code></li> <li>Support for various image formats (JPG, PNG, TIFF, etc.)</li> </ul>"},{"location":"imagefolder-guide/#example-dataset","title":"Example Dataset","text":"<p>This guide demonstrates working with the NEON Beetle dataset, which contains high-resolution images of beetles collected by the National Ecological Observatory Network.</p>"},{"location":"imagefolder-guide/#basic-usage","title":"Basic Usage","text":""},{"location":"imagefolder-guide/#simple-setup","title":"Simple Setup","text":"<pre><code>from hpc_inference.datasets import ImageFolderDataset\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\n\n# Define transforms\ntransform = transforms.Compose([\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n                        std=[0.229, 0.224, 0.225])\n])\n\n# Create dataset\ndataset = ImageFolderDataset(\n    root_dir=\"/path/to/beetle/images\",\n    transform=transform,\n    extensions=('.jpg', '.jpeg', '.png', '.tiff')\n)\n\n# Create dataloader\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    num_workers=4,\n    pin_memory=True,\n    shuffle=False\n)\n</code></pre>"},{"location":"imagefolder-guide/#processing-images","title":"Processing Images","text":"<pre><code>import torch\n\n# Process all images\nfor batch_idx, (images, paths) in enumerate(dataloader):\n    # Move to GPU if available\n    if torch.cuda.is_available():\n        images = images.cuda()\n\n    # Your inference code here\n    with torch.no_grad():\n        outputs = model(images)\n\n    # Process outputs\n    print(f\"Batch {batch_idx}: Processed {len(images)} images\")\n    for i, path in enumerate(paths):\n        print(f\"  {path}: {outputs[i].shape}\")\n</code></pre>"},{"location":"imagefolder-guide/#advanced-features","title":"Advanced Features","text":""},{"location":"imagefolder-guide/#multi-model-preprocessing","title":"Multi-Model Preprocessing","text":"<p>TODO</p>"},{"location":"imagefolder-guide/#validation-and-error-handling","title":"Validation and Error Handling","text":"<p>TODO</p>"},{"location":"imagefolder-guide/#distributed-processing","title":"Distributed Processing","text":""},{"location":"imagefolder-guide/#multi-gpu-setup","title":"Multi-GPU Setup","text":"<p>TODO</p>"},{"location":"imagefolder-guide/#performance-optimization","title":"Performance Optimization","text":""},{"location":"imagefolder-guide/#memory-management","title":"Memory Management","text":"<pre><code># Optimize memory usage\ndataloader = DataLoader(\n    dataset,\n    batch_size=32,\n    num_workers=4,\n    pin_memory=True,\n    persistent_workers=True,  # Keep workers alive between epochs\n    prefetch_factor=2         # Prefetch batches per worker\n)\n</code></pre>"},{"location":"imagefolder-guide/#profiling","title":"Profiling","text":"<pre><code>import time\nfrom contextlib import contextmanager\n\n@contextmanager\ndef timer(name):\n    start = time.time()\n    yield\n    end = time.time()\n    print(f\"{name}: {end - start:.4f} seconds\")\n\n# Profile data loading\nwith timer(\"Data loading\"):\n    for batch_idx, (images, paths) in enumerate(dataloader):\n        if batch_idx &gt;= 10:  # Profile first 10 batches\n            break\n\n        with timer(f\"Batch {batch_idx}\"):\n            # Your processing code\n            pass\n</code></pre>"},{"location":"imagefolder-guide/#best-practices","title":"Best Practices","text":""},{"location":"imagefolder-guide/#1-choose-appropriate-batch-size","title":"1. Choose Appropriate Batch Size","text":"<ul> <li>Start with batch size 16 and adjust based on GPU memory</li> <li>Larger batches generally improve GPU utilization</li> <li>Monitor memory usage to avoid OOM errors</li> </ul>"},{"location":"imagefolder-guide/#2-optimize-number-of-workers","title":"2. Optimize Number of Workers","text":"<ul> <li>Start with <code>num_workers = num_gpus</code></li> <li>Monitor CPU usage to find optimal value</li> <li>Too many workers can cause overhead</li> </ul>"},{"location":"imagefolder-guide/#3-use-pin-memory","title":"3. Use Pin Memory","text":"<ul> <li>Enable <code>pin_memory=True</code> for GPU processing</li> <li>Speeds up data transfer to GPU</li> </ul>"},{"location":"imagefolder-guide/#4-handle-corrupted-files","title":"4. Handle Corrupted Files","text":"<ul> <li>Always validate your dataset before processing to avoid unexpected job crashing</li> <li>Implement error handling in your data loading pipeline</li> <li>Log corrupted files for investigation</li> </ul>"},{"location":"imagefolder-guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"imagefolder-guide/#common-issues","title":"Common Issues","text":"<ol> <li>Out of Memory Errors</li> <li>Reduce batch size</li> <li>Reduce number of workers</li> <li> <p>Use gradient checkpointing</p> </li> <li> <p>Slow Data Loading</p> </li> <li>Increase number of workers</li> <li>Use faster storage (SSD vs HDD)</li> <li> <p>Optimize image formats</p> </li> <li> <p>CUDA Errors</p> </li> <li>Ensure CUDA compatibility</li> <li>Check GPU memory usage</li> <li>Verify data types and shapes</li> </ol>"},{"location":"utilization-metrics/","title":"GPU Utilization Metrics","text":""},{"location":"utilization-metrics/#key-desiderata-for-a-gpu-utilization-statistic","title":"Key Desiderata for a GPU Utilization Statistic","text":"<p>An ideal metric for GPU utilization should reward high average usage, penalize low/idle time, and account for stability (low variance).</p> <ol> <li>High mean utilization is good.</li> <li>Low variance (stable, not spiky) is good.</li> <li>Duration/time-normalization is important\u2014long idle periods should penalize the score.</li> <li>Interpretable on [0, 1] or [0%, 100%] scale if possible.</li> </ol>"},{"location":"utilization-metrics/#basic-statistics","title":"Basic Statistics","text":"<p>Let \\(u_1, u_2, ..., u_n\\) be the sequence of GPU utilization percentages (sampled at regular intervals, in [0, 100]).</p>"},{"location":"utilization-metrics/#a-mean-utilization","title":"A. Mean Utilization","text":"\\[ \\mu_u = \\frac{1}{n} \\sum_{i=1}^n u_i \\] <ul> <li>Pros: Simple, intuitive.</li> <li>Cons: Can be misleading if you have brief spikes and lots of idle periods.</li> </ul>"},{"location":"utilization-metrics/#b-standard-deviation-variance","title":"B. Standard Deviation (Variance)","text":"\\[ \\sigma_u = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n (u_i - \\mu_u)^2 } \\] <ul> <li>High \u03c3: Utilization is unstable/spiky.</li> </ul>"},{"location":"utilization-metrics/#c-proposed-robust-utilization-metric","title":"C. Proposed Robust Utilization Metric","text":""},{"location":"utilization-metrics/#1-effective-utilization-mean-std","title":"1. \u201cEffective Utilization\u201d (Mean - \u03bb \u00d7 Std)","text":"\\[ \\text{EffU} = \\mu_u - \\lambda \\sigma_u \\] <ul> <li>Where \u03bb is a tradeoff factor (e.g., \u03bb=1).</li> <li>Interpretation: Rewards high mean, penalizes high variance.</li> </ul>"},{"location":"utilization-metrics/#2-fraction-of-time-above-threshold","title":"2. Fraction of Time Above Threshold","text":"\\[ \\text{Frac}_{\\theta} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{ u_i &gt; \\theta \\} \\] <ul> <li>e.g., \u03b8=80%.</li> <li>What fraction of the run is \u201chighly utilized\u201d?</li> </ul>"},{"location":"utilization-metrics/#3-area-under-the-utilization-curve-auc","title":"3. Area Under the Utilization Curve (AUC)","text":"\\[ \\text{AUC}_u = \\frac{1}{100 n} \\sum_{i=1}^n u_i \\] <ul> <li>Same as mean, but normalized to [0,1].</li> <li>AUC is also robust if your sampling interval is uniform.</li> <li>AUC only reflects \u201caverage work done,\u201d not how the work was distributed in time. For hardware optimization and system diagnosis, you also want to know if the workload is steady or bursty, and how often the GPU is left waiting.</li> </ul>"},{"location":"utilization-metrics/#d-composite-gpu-efficiency-score","title":"D. Composite \u201cGPU Efficiency Score\u201d","text":"<p>Let\u2019s define a simple composite metric:</p> \\[ \\text{GPU Efficiency} = \\frac{\\text{Mean Util} - \\sigma_u}{100} \\] <ul> <li>Range: Can be negative (bad) or up to 1 (perfect: mean=100, std=0).</li> <li> <p>Interpretation:</p> </li> <li> <p>1.0: Always 100%, perfectly steady.</p> </li> <li>0.8: Average 90%, std 10.</li> <li>Negative: mean is low and/or std is very high (spiky/idle).</li> </ul> <p>Or:</p> \\[ \\text{GPU Utilization Score} = \\frac{1}{100} \\left( \\alpha \\cdot \\mu_u + (1 - \\alpha) \\cdot \\text{Frac}_{\\theta} \\right) \\] <p>Where \u03b1 is a weight (e.g., 0.5), \u03b8 is a high-utilization threshold (e.g., 80%).</p>"},{"location":"utilization-metrics/#e-time-weighted-adjustment-if-needed","title":"E. Time-Weighted Adjustment (if needed)","text":"<p>If intervals are not uniform, multiply each utilization by its interval and divide by total time:</p> \\[ \\text{TimeWeightedMean} = \\frac{ \\sum_{i=1}^n u_i \\Delta t_i }{ \\sum_{i=1}^n \\Delta t_i } \\]"},{"location":"utilization-metrics/#critiquelimitations","title":"Critique/Limitations","text":"<ul> <li>High mean but high variance may indicate batchiness, pipeline stalling\u2014lower score with the above metric.</li> <li>High mean with low std is truly optimal (score near 1).</li> <li>Low mean and low std means consistently idle (score near 0 or negative).</li> <li>Composite metrics can be tuned (\u03bb or \u03b1) to emphasize stability or average, depending on workload.</li> </ul>"},{"location":"utilization_stat/","title":"Utilization stat","text":""},{"location":"utilization_stat/#key-desiderata-for-a-gpu-utilization-statistic","title":"Key Desiderata for a GPU Utilization Statistic","text":"<p>An ideal metric for GPU utilization should reward high average usage, penalize low/idle time, and account for stability (low variance).</p> <ol> <li>High mean utilization is good.</li> <li>Low variance (stable, not spiky) is good.</li> <li>Duration/time-normalization is important\u2014long idle periods should penalize the score.</li> <li>Interpretable on [0, 1] or [0%, 100%] scale if possible.</li> </ol>"},{"location":"utilization_stat/#basic-statistics","title":"Basic Statistics","text":"<p>Let \\(u_1, u_2, ..., u_n\\) be the sequence of GPU utilization percentages (sampled at regular intervals, in [0, 100]).</p>"},{"location":"utilization_stat/#a-mean-utilization","title":"A. Mean Utilization","text":"\\[ \\mu_u = \\frac{1}{n} \\sum_{i=1}^n u_i \\] <ul> <li>Pros: Simple, intuitive.</li> <li>Cons: Can be misleading if you have brief spikes and lots of idle periods.</li> </ul>"},{"location":"utilization_stat/#b-standard-deviation-variance","title":"B. Standard Deviation (Variance)","text":"\\[ \\sigma_u = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^n (u_i - \\mu_u)^2 } \\] <ul> <li>High \u03c3: Utilization is unstable/spiky.</li> </ul>"},{"location":"utilization_stat/#c-proposed-robust-utilization-metric","title":"C. Proposed Robust Utilization Metric","text":""},{"location":"utilization_stat/#1-effective-utilization-mean-std","title":"1. \u201cEffective Utilization\u201d (Mean - \u03bb \u00d7 Std)","text":"\\[ \\text{EffU} = \\mu_u - \\lambda \\sigma_u \\] <ul> <li>Where \u03bb is a tradeoff factor (e.g., \u03bb=1).</li> <li>Interpretation: Rewards high mean, penalizes high variance.</li> </ul>"},{"location":"utilization_stat/#2-fraction-of-time-above-threshold","title":"2. Fraction of Time Above Threshold","text":"\\[ \\text{Frac}_{\\theta} = \\frac{1}{n} \\sum_{i=1}^n \\mathbf{1}\\{ u_i &gt; \\theta \\} \\] <ul> <li>e.g., \u03b8=80%.</li> <li>What fraction of the run is \u201chighly utilized\u201d?</li> </ul>"},{"location":"utilization_stat/#3-area-under-the-utilization-curve-auc","title":"3. Area Under the Utilization Curve (AUC)","text":"\\[ \\text{AUC}_u = \\frac{1}{100 n} \\sum_{i=1}^n u_i \\] <ul> <li>Same as mean, but normalized to [0,1].</li> <li>AUC is also robust if your sampling interval is uniform.</li> <li>AUC only reflects \u201caverage work done,\u201d not how the work was distributed in time. For hardware optimization and system diagnosis, you also want to know if the workload is steady or bursty, and how often the GPU is left waiting.</li> </ul>"},{"location":"utilization_stat/#d-composite-gpu-efficiency-score","title":"D. Composite \u201cGPU Efficiency Score\u201d","text":"<p>Let\u2019s define a simple composite metric:</p> \\[ \\text{GPU Efficiency} = \\frac{\\text{Mean Util} - \\sigma_u}{100} \\] <ul> <li>Range: Can be negative (bad) or up to 1 (perfect: mean=100, std=0).</li> <li> <p>Interpretation:</p> </li> <li> <p>1.0: Always 100%, perfectly steady.</p> </li> <li>0.8: Average 90%, std 10.</li> <li>Negative: mean is low and/or std is very high (spiky/idle).</li> </ul> <p>Or:</p> \\[ \\text{GPU Utilization Score} = \\frac{1}{100} \\left( \\alpha \\cdot \\mu_u + (1 - \\alpha) \\cdot \\text{Frac}_{\\theta} \\right) \\] <p>Where \u03b1 is a weight (e.g., 0.5), \u03b8 is a high-utilization threshold (e.g., 80%).</p>"},{"location":"utilization_stat/#e-time-weighted-adjustment-if-needed","title":"E. Time-Weighted Adjustment (if needed)","text":"<p>If intervals are not uniform, multiply each utilization by its interval and divide by total time:</p> \\[ \\text{TimeWeightedMean} = \\frac{ \\sum_{i=1}^n u_i \\Delta t_i }{ \\sum_{i=1}^n \\Delta t_i } \\]"},{"location":"utilization_stat/#f-example-in-python","title":"F. Example in Python","text":"<pre><code>import numpy as np\n\ngpu_util = np.array([...])  # Your utilization sequence (0-100)\nmean_util = gpu_util.mean()\nstd_util = gpu_util.std()\nfrac_high = np.mean(gpu_util &gt; 80)\ngpu_efficiency = (mean_util - std_util) / 100\nauc = gpu_util.mean() / 100\n\nprint(f\"Mean Util: {mean_util:.1f}%\")\nprint(f\"Std Util: {std_util:.1f}\")\nprint(f\"Fraction &gt; 80%: {frac_high:.2f}\")\nprint(f\"GPU Efficiency: {gpu_efficiency:.3f}\")\nprint(f\"AUC (normalized): {auc:.3f}\")\n</code></pre>"},{"location":"utilization_stat/#critiquelimitations","title":"Critique/Limitations","text":"<ul> <li>High mean but high variance may indicate batchiness, pipeline stalling\u2014lower score with the above metric.</li> <li>High mean with low std is truly optimal (score near 1).</li> <li>Low mean and low std means consistently idle (score near 0 or negative).</li> <li>Composite metrics can be tuned (\u03bb or \u03b1) to emphasize stability or average, depending on workload.</li> </ul>"}]}